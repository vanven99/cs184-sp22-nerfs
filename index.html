<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    body {
      padding: 100px;
      max-width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }

    h1,
    h2,
    h3,
    h4 {
      font-family: 'Roboto Condensed', sans-serif;
    }

    h2,
    h3,
    h4 {
      font-style: none;
      font-weight: none;
    }

    #table {
      background: white;
      color: rgb(36, 36, 36);
      font-size: 12pt;
      border-collapse: collapse;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      margin: 0 auto;
    }

    #table tr:nth-child(even) {
      background-color: #f2f2f2;
    }

    #table tbody tr:hover {
      background-color: #ddd;
    }

    #table tfoot tr:hover {
      background-color: #ddd;
    }

    .flexbox {
      display: flex;
      justify-content: space-around;
      max-width: 100%;
    }

    .flex-row {
      flex-flow: row wrap;
    }

    .flex-col {
      flex-flow: column;
    }

    .link:hover {
      color: rgb(53, 107, 255);
      animation-duration: .25s;
      cursor: pointer;
    }

    .invisible-a {
      color: black;
      text-decoration: none;
    }

    .title-img {
      margin: 2px;
    }

    a {
      color: rgb(53, 107, 255);
    }

    .pb {
      padding-bottom: 20px;
    }

    .pb-sm {
      padding-bottom: 10px;
    }

    .math {
      max-width: 1000px;
    }

    .section {
      margin-bottom: 40px;
    }

    .img-table {
      border-collapse: collapse;
      border-spacing: 0px;
    }

    .img-table tr {
      margin: 0px;
      padding: 0px;
      border-collapse: collapse;
    }

    .img-table td {
      margin: 0px;
      padding: 0px;
      border-collapse: collapse;
      border: solid white 4px;
    }

    .img-table img {
      display: block;
    }

    /* Styled table source: https://dev.to/dcodeyt/creating-beautiful-html-tables-with-css-428l */
    .styled-table {
      border-collapse: collapse;
      margin: 25px 0;
      font-size: 0.9em;
      min-width: 400px;
    }

    .styled-table thead tr {
      background-color: #565966;
      color: #ffffff;
      text-align: left;
      font-weight: normal;
    }

    .styled-table th,
    .styled-table td {
      padding: 12px 15px;
    }

    .styled-table tbody tr {
      border-bottom: 1px solid #dddddd;
    }
  </style>
  <title>VRNeRFs - CS 184 Sp22 Final Project</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Condensed:wght@300&display=swap" rel="stylesheet">
  <link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css'>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <script type="text/x-mathjax-config">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ["\\(", "\\)"]],
        processEscapes: true,
      }
    }
  </script>

  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" type="text/css" href="http://tikzjax.com/v1/fonts.css">

  <script src="http://tikzjax.com/v1/tikzjax.js"></script>

</head>

<body>

  <h1 align="middle" id="title">VRNeRFs</h1>
  <h2 align="middle">A VR Viewer for Neural Radiance Fields</h2>
  <h3 align="middle">
    Team 14 <br />
    <div class="flexbox flex-row">
      <div>Abhik Ahuja</div>
      <div>Cyrus Hamirani</div>
      <div>Michael Van Luven</div>
      <div>Gavin Fure</div>
    </div>
  </h3>
  <div class="flexbox" href="https://github.com/vanven99/cs184-sp22-nerfs/">
    <a class="invisible-a" href="https://github.com/vanven99/cs184-sp22-nerfs/">
      <div class="link">
        <i class='fa fa-github fa-3x'></i>
        <div>Code</div>
      </div>
    </a>
  </div>

  <br><br>

  <div>

    <div class="flexbox pb">
      <img class="title-img" src="./images/foxstereo.gif" width="100%">
      <img class="title-img" src="./images/ficusmono.gif" width="100%">
      <img class="title-img" src="./images/andrewmono.gif" width="100%">
      <img class="title-img" src="./images/carstereo.gif" width="100%">
    </div>
    <table width="100%">
      <tr>
      </tr>
    </table>
    <div align="middle">
      <h2>Video</h2>
      <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/5Xjh8SbdhSE"
        title="YouTube video player" frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen></iframe>
    </div>
    <h2 align="middle">Abstract</h2>
    <div class="section">
      <p>
        For our final project, we created a virtual reality viewer for scenes generated from neural radiance fields
        (NeRFs). Our inspiration for this project came from our curiosity about the possible use cases of NeRF in user
        applications, in addition to its performance. We thought that building a VR viewer for NeRFs was a
        natural choice, as a VR application would allow a user to view the scenes generated by a NeRF in a highly
        immersive environment.
      </p>
    </div>
    <h2 align="middle">Technical Summary</h2>
    <div class="section">
      <p>
        To create this application, we began with two major already-existing code libraries: Instant-NGP and OpenVR.
        OpenVR is a code library that allows you to communicate with and display graphics to a virtual reality
        head-mounted display. Instant-NGP is a high-performance graphics library that includes code to train and query
        images from a NeRF. We chose Instant-NGP over other NeRF Renderers for its performance and Python
        API.
      </p>
      <p>
        The basic pipeline of our NeRF VR viewer is as follows:
      </p>
      <center>
        <img src="./images/basicpipeline.PNG">
      </center>
      <p>
        During runtime, an OpenVR script initializes and obtains the headset’s current position and orientation in
        space. It sends this data as a camera matrix over a named pipe to our Instant-NGP script. To use this camera
        matrix, we first resolve the differences between the Instant-NGP and OpenVR camera coordinate systems.
        Instant-NGP then generates an image from a trained NeRF model and sends the queried image as bytes to the VR
        headset. This image data is interpreted by our OpenVR script and is used to create an OpenGL texture, which is
        then applied to a VR overlay (or, in the case of stereo, submitted to both eyes via the OpenVR compositor). This
        overlay is placed in front of the user’s eyes in VR.
      </p>
      <p>
        This entire process repeats in a loop until the program is terminated.
      </p>
    </div>
    <h2 align="middle">Technical Details - Named Pipes</h2>
    <div class="section">
      <p>
        To enable interprocess communication we decided to use named pipes, which allow programs to read and send bytes
        in a server-client relationship. During runtime the OpenVR script and Instant-NGP engage in a connection
        handshake, ensuring that both programs are ready to receive and send their respective information. This
        handshake is modeled below.
      </p>
      <center>
        <img src="./images/pipeack.PNG">
      </center>
      <p>
        It is crucial that the OpenVR script waits for a client connection from Instant-NGP before connecting to the
        Instant-NGP server. Named pipes will throw an error if attempting to connect to a non-existent server. The data
        must also be converted into a byte format when sent through the pipe, then re-converted back into a usable
        format in the respective programs.
      </p>
    </div>
    <h2 align="middle">Technical Details - Camera Data Transformation/Conversion</h2>

    <div class="section">
      <p>
        OpenVR and Instant-NGP use different coordinate systems. In OpenVR, the camera is oriented toward the negative z
        direction, positive x is towards the right, and positive y faces up. However, in Instant-NGP, the coordinate
        system has the z axis reversed relative to OpenVR. Therefore, we had to find a way to transform
        between the two systems to ensure that we could accurately track the Instant-NGP camera’s movements to the
        movements of the VR headset.
      </p>

      <p>
        Both systems use similar matrix conventions to define the camera positions; they use 3x4 matrices where the
        first 3 columns define the x, y, z rotation and the 4th column describes an xyz translation from the origin.
      </p>
      <p>
        $$\begin{bmatrix}
        \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & x_{pos}\\
        X_{rot} & Y_{rot} & Z_{rot} & y_{pos}\\
        \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & z_{pos}\\
        \end{bmatrix}$$
      </p>

      <p>
        When transforming from headset-space to NeRF-space coordinates, we first need to negate the relevant axes so
        both systems move in the same direction. Given the headset position matrix, we can do this by negating the
        translation vector, as well as the relevant rotation columns. However, in implementation, the Instant-NGP code
        processes the camera position matrix by negating certain rotation vectors. Therefore, we found that we only
        needed to negate the x and z rotation vectors to correct these axes. We were able to achieve these negations by
        multiplying the headset position elementwise with a negation matrix:
      </p>
      <p>
        $$ \begin{bmatrix}
        -1 & 1 & -1 & -1\\
        -1 & 1 & -1 & -1\\
        -1 & 1 & -1 & -1
        \end{bmatrix} $$

      </p>
      We then wanted to make sure that the viewer always
      started in the same orientation in NeRF-space. We started by defining an initial camera matrix
      that we thought was a good starting point. For us, that was:
      </p>

      <p>
        $$ \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & -1 & 0 & 0\\
        0 & 0 & 1 & 0
        \end{bmatrix} $$
      </p>
      <p>
        where only the first 3 columns represent the rotation. Then, using \(H\) to represent the initial headset
        rotation and \(C_{init}\) as the initial camera rotation, we can simply find a transform between headset space
        and NeRF space:
      </p>
      <p>
        $$
        X\cdot H = C_{init}$$
        $$
        X\cdot H\cdot H^{-1} = C_{init}\cdot H^{-1} \\
        $$
        $$
        X = C_{init}\cdot H^{-1} \\
        $$
      </p>
      <p>
        For every subsequent headset position \(H_t\), we multiply \(X \cdot H_t\) to get the headset rotation in camera space.
      </p>
      <p>
        To calculate the translation, we multiply the headset’s xyz position by a passed in scaling factor to account
        for the difference in scale between the headset coordinate system and the NeRFs. We rotate it using the same
        transformation matrix \(X\) to acount for the difference in coordinate systems. We then concatenate this to the
        calculated headset rotation in camera-space.
      </p>
      <p>
        $$
        \begin{bmatrix}
        & x_{pos} * t\\
        X\cdot H & y_{pos} * t\\
        & z_{pos}*t
        \end{bmatrix}
        $$
      </p>
      <p>
        For one final implementation detail, when processing the view matrix, the Instant-NGP code cycles the rows up by
        one row in order to convert the matrix into a format understandable by the inner NeRF code. Therefore, we cycle
        our matrix down by one row to keep the axes in the correct order after processing.
      </p>
      <p>
        $$
        \begin{bmatrix}
        X\cdot H[2] & z_{pos}*t\\
        X\cdot H[0] & y_{pos} * t\\
        X\cdot H[1] & x_{pos} * t
        \end{bmatrix}
        $$
      </p>
      <br>
      <h2 align="middle">Technical Details - Stereo Imaging</h2>
      <p>
        To implement stereo imaging, we must calculate a view matrix for each eye and use them to render an image from
        each eye. We can then send those images to the headset to be viewed as a stereo pair.
      </p>
      <p>
        We first use the OpenVR method getEyeToHeadTransform() to find the position of each eye relative to the headset
        position in object space. We then rotate these eye offsets by the headset rotation and add them directly to the
        headset position to get the eye positions in world space.
      </p>
      <p>
        To achieve the effect of stereo 3D, we need to converge the view direction of each eye to focus on a common
        point. This is accomplished by adding a parameter for the focusing distance of the eyes, \(x\). Knowing the
        Interpupillary Distance, \(IPD\), we can set up a diagram representing the view direction of the eyes. Here,
        \(f\) is the point at which the eyes are focused, \(\theta\) is the amount by which the eyes must be rotated,
        and \(l\) and \(r\) are the left and right eye positions respectively.
      </p>
      <center>
        <img src="./images/IPD.PNG" width="300px">
      </center>
      <p>
        We can then solve for \(\theta\) using triangle equations:
      </p>
      <p>
        $$\theta = \arcsin{(\frac{IPD/2}{\sqrt{(IPD/2)^2 + x^2}})}$$
      </p>
      <p>
        To converge the eyes we multiply the rotation of the left eye by a rotation matrix about the y axis of
        \(-\theta\) and the right eye by \(\theta\).
      </p>
      <p>
        To sync the image for each eye we made sure that the images for each eye were rendered using the same headset
        pose. On program startup the initial headset pose is first sent to Instant-NGP to calculate the transform
        matrix. Both eye positions are then calculated and sent to Instant-NGP. These are used to render two images
        which are sent to OpenVR, and this process continues until program termination.
      </p>
    </div>

    <h2 align="middle">Technical Details - OpenGL Texturing</h2>
    <div class="section">
      <p>
        To use the image bytes obtained from Instant-NGP, the image must first be converted into an OpenGL texture.
        First an OpenGL context is defined to store the state associated with the OpenGL instance. Then the byte array
        is converted into a usable structure utilizing multiple for loops. This creates a nested list structure, where
        the innermost list corresponds to the pixel data, the next list is the image row, and the outermost list is the
        complete image. The alpha values are discarded during this step.
      </p>
      <p class="math">
        $$
        \begin{bmatrix}
        r_{1} & g_{1} & b_{1} & a_{1} & \dots & r_{w*h} & g_{w*h} & b_{w*h} &
        a_{w*h} \\
        \end{bmatrix}
        \to \begin{bmatrix}

        \begin{bmatrix}

        \begin{bmatrix}
        r_{0} & g_{0} & b_{0}\\
        \end{bmatrix}
        &
        \dots
        &
        \begin{bmatrix}
        r_{w - 1} \hspace1ex & g_{w-1} \hspace1ex & b_{w-1}\\
        \end{bmatrix}\\
        \end{bmatrix}\\
        \begin{bmatrix}

        \begin{bmatrix}
        r_{w} & g_{w} & b_{w}\\
        \end{bmatrix}
        &
        \dots
        &
        \begin{bmatrix}
        r_{2*w - 1} & g_{2*w - 1} & b_{2*w - 1}\\
        \end{bmatrix}
        \end{bmatrix}\\

        \\
        \vdots
        \\
        \\
        \begin{bmatrix}
        \begin{bmatrix}
        r_{w*(h-1)} & g_{w*(h-1)} & b_{w*(h-1)}\\
        \end{bmatrix}
        &
        \dots
        &
        \begin{bmatrix}
        r_{w*h} & g_{w*h} & b_{w*h}\\
        \end{bmatrix}
        \end{bmatrix}\\


        \end{bmatrix}
        $$
      </p>
      <p>
        Converting the reorganized data into an OpenGL texture is achieved by calling glTexImage2D() on the image bytes
        and binding the texture to an OpenGL ID. We account for minification and magnification cases by using a linear
        sampler.
      </p>
    </div>

    <h2 align="middle">Technical Details - VR Overlay/Compositor</h2>
    <div class="section">
      <p>
        A VR overlay represents a 2D plane set in the 3D scene. In mono-imaging, the NeRF scene is rendered onto a 2D
        overlay that is positioned to be right in front of the viewing angle. The overlay follows the user’s movements
        exactly, appearing at the same place in their field of view at each frame.
      </p>
      <p>
        To project the image onto the overlay we call the OpenVR function SetOverlayTexture() which applies the texture
        generated in the previous step to an overlay plane. Then to position the overlay correctly we call
        SetOverlayTransformTrackedDeviceRelative(), which applies a transform to the overlay relative to the headset.
      </p>
      <p>
        We define a transform that places the overlay just in front of the user’s eyeline at all times. Because the
        transform is relative to the head, the overlay plane appears in the same spot in the user’s field of view in
        every frame. Since the 2D image changes in response to user movements in a way that is consistent with 3D
        perspective, it creates the illusion of a 3D scene.
      </p>
      <p>
        In stereo imaging, we instead submit an image for each eye to OpenVR’s compositor. Instead of applying a texture
        to an overlay for the user to view, we apply the texture directly onto each eye display. This allows for two
        different textures to be viewed simultaneously. Using the compositor does come with some drawbacks, notably if
        the texture is not updated at a high enough frame rate it will begin flickering.
      </p>
    </div>

    <h2 align="middle">Final Results</h2>

    <div class="section"></div>
    <h3 align="middle">Benchmarks</h3>
    <div class="pb-sm">
      <table id="table" class="styled-table">
        <thead>
          <tr>
            <th>Scene</th>
            <th>Mono</th>
            <th>Stereo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Lego Car</td>
            <td>~4 fps</td>
            <td>~2 fps</td>
          </tr>
          <tr>
            <td>Ficus</td>
            <td>~9 fps</td>
            <td>~4 fps</td>
          </tr>
          <tr>
            <td>Desk</td>
            <td>~3-4 fps</td>
            <td>~1-2 fps</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h3 align="middle">Mono</h3>
    <div class="pb-sm">
      <table class="img-table" width="100%">
        <tr>
          <td> <img src="./images/carmono.gif" width="100%"> </td>
          <td> <img src="./images/ficusmono.gif" width="100%"> </td>
          <td> <img src="./images/foxmono.gif" width="100%"> </td>
        </tr>
        <tr>
          <td> <img src="./images/abhikmono.gif" width="100%"> </td>
          <td> <img src="./images/cyrusmono.gif" width="100%"> </td>
          <td> <img src="./images/andrewmono.gif" width="100%"> </td>
        </tr>
      </table>
      <em style="margin-top: 10px;">Above: A series of VR views where a single image is rendered per frame and displayed to both eyes. The ficus (top middle) and fox (top right) scenes are from the Instant-NGP and NeRF datasets. The others are trained from our own data.</em>
    </div>

    <h3 align="middle">Stereo</h3>
    <table class="img-table" width="100%">
      <tr>
        <td> <img src="./images/carstereo.gif" width="100%"> </td>
        <td> <img src="./images/ficusstereo.gif" width="100%"> </td>
        <td> <img src="./images/foxstereo.gif" width="100%"> </td>
      </tr>
      <tr>
        <td> <img src="./images/abhikstereo.gif" width="100%"> </td>
        <td> <img src="./images/cyrusstereo.gif" width="100%"> </td>
        <td> <img src="./images/andrewstereo.gif" width="100%"> </td>
      </tr>
    </table>
      <em style="margin-top: 10px;">Above: A series of VR views where an image is rendered per eye and displayed to create a stereo depth effect. The ficus (top middle) and fox (top right) scenes are from the Instant-NGP and NeRF datasets. The others are trained from our own data.</em>
  </div>

  <h2 align="middle">Lessons Learned / Issues Faced</h2>
  <div class="section">
    <p>
      We learned that we should ensure that multiple group members have access to equipment that can run our project.
      At first, only one of our computers was capable of running Instant-NGP, and we only had one VR headset. This
      reduced our capability to work in parallel / asynchronously.
    </p>
    <p>
      We also learned that thorough reading of documentation is invaluable! Many issues that we ran into were solved
      simply by taking a step back, reading the documentation to understand what we were working with, and going from
      there. For example, at one point we were stuck on getting OpenGL to properly convert the image data sent from
      InstantNGP into a texture. By reading documentation from both OpenGL and InstantNGP, we figured out that the
      issue was the format for the pixel data not matching. We were quickly able to fix this issue once we realized
      it, thanks to the documentation.
    </p>
  </div>

  <h2 align="middle">Future Work</h2>
  <div class="section">
    <p>
      Light field rendering - One way to speed up performance could be extracting a learned lightfield
      from a NeRF, and rendering views from that, rather than continually querying the NeRF for image data.
    </p>
    <p>
      Adaptive Focusing - Currently, our stereo works by orienting the eyes to focus at a fixed distance d, where d is
      a passed in parameter. Future work could instead adaptively shift focus to different distances over time as the
      user looks at different objects in space.
    </p>
  </div>

  <h2 align="middle">References</h2>
  <div class="section">
    <ul>
      <li>
        <a href="https://www.khronos.org/opengl/wiki/Creating_an_OpenGL_Context_(WGL)">OpenGL Context Guide</a><br>
      </li>
      <li>
        <a href="https://github.com/NVlabs/instant-ngp">Instant-NGP</a><br>
      </li>
      <li>
        <a href="https://github.com/ValveSoftware/openvr">OpenVR Library and Documentation</a><br>
      </li>
      <li>
        <a href="https://github.com/cmbruns/pyopenvr">PyOpenVR API</a><br>
      </li>
    </ul>
  </div>
  <h2 align="middle">Contributions</h2>
  <p>
    Abhik Ahuja - Contributed to coordinate system tranformations, stereo imaging, codebase management,
    refinements and additional tooling, final writeup, video.
  </p>
  <p>
    Cyrus Hamirani - Contributed to OpenGL texture work, interfacing with InstantNGP’s API, working with OpenVR
    (generating overlays, compositor, etc), stereo imaging, final writeup, video.
  </p>
  <p>
    Michael Van Luven - Contributed to OpenGL texture creation, named pipes, interfacing with InstantNGP API,
    working with OpenVR (generating overlays, compositor, etc), mono imaging, and camera coordinate system, final
    writeup, video.
  </p>
  <p>
    Gavin Fure - Contributed to OpenGL textures and overlays, camera coordinate transformations, documentation
    research, final writeup, testing software, voiceover.
  </p>


</body>

</html>