<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  #table {
  background: white;
  color: black;
  font-size: 12pt;
  border-collapse: collapse;
  text-align: left;
  font-weight: 300;
  font-family: 'Open Sans', sans-serif;
  margin: 0 auto;
}
#table tr:nth-child(even){background-color: #f2f2f2;}
#table tbody tr:hover {background-color: #ddd;}
#table tfoot tr:hover {background-color: #ddd;}
</style>
<title>CS 184 Spring 2022 Final Project</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script type="text/x-mathjax-config">
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ["\\(", "\\)"]],
      processEscapes: true,
    }
  }
</script>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" type="text/css" href="http://tikzjax.com/v1/fonts.css">

<script src="http://tikzjax.com/v1/tikzjax.js"></script>


</head>


<body>

<h1 align="middle">Neural Radiance Field with Virtual Reality</h1>
<h2 align="middle">Team Members: Abhik Ahuja, Cyrus Hamirani, Michael Van Luven, Gavin Fure</h2>

<br><br>

<div>

<h3 align="middle">
  <a href="https://docs.google.com/presentation/d/1b6wWeYN4B_NU59R86-OZxvLjvAj_eW9BAFThMZKsrTQ/edit?usp=sharing">Link to slides</a><br>
  <a href="https://youtu.be/YfVELl7ZDqQ">Link to video</a><br>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/YfVELl7ZDqQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</h3>

<h2 align="middle">Abstract</h2>
<p>
  For our final project, we created a virtual reality viewer for scenes generated from neural radiance fields (NeRFs). Our inspiration for this project came from our curiosity about the possible use cases of NeRF in user applications, in addition to its performance capabilities. We thought that building a VR viewer for NeRFs was a natural choice, as a VR application would allow a user to view the scenes generated by a NeRF in a highly immersive environment.
</p>
<br>
<h2 align="middle">Technical Summary</h2>
<p>
To create this application, we began with two major already-existing code libraries: Instant-NGP and OpenVR. OpenVR is a code library that allows you to communicate with and display graphics to a virtual reality head-mounted display. Instant-NGP is a high-performance graphics library that includes code to train and query images from a NeRF. We chose Instant-NGP over other NeRF Renderers for its performance capabilities and Python API.
</p>
<p>
  The basic pipeline of our NeRF VR viewer is as follows:
</p>
<center>
  <img src="./images/basicpipeline.PNG">
</center>
<p>
  During runtime, an OpenVR script initializes and obtains the headset’s current position and orientation in space. It sends this data as a camera matrix over a named pipe to our Instant-NGP script. To use this camera matrix, we first resolve the differences between the Instant-NGP and OpenVR camera coordinate systems. Instant-NGP then generates an image from a trained NeRF model and sends the queried image as bytes to the VR headset. This image data is interpreted by our OpenVR script and is used to create an OpenGL texture, which is then applied to a VR overlay (or, in the case of stereo, submitted to both eyes via the OpeNVR compositor). This overlay is placed in front of the user’s eyes in VR.
</p>
<p>
  This entire process repeats in a loop until the program is terminated.
</p>
<br>
<h2 align="middle">Technical Details - Named Pipes</h2>
<p>
  To enable interprocess communication we decided to use named pipes, which allows programs to read and send bytes in a server-client relationship. During runtime the OpenVR script and Instant-NGP engage in a connection handshake, ensuring that both programs are ready to receive and send their respective information. This handshake is modeled below.
</p>
<center>
  <img src="./images/pipeack.PNG">
</center>
<p>
   It is crucial that the OpenVR script waits for a client connection from Instant-NGP before connecting to the Instant-NGP server. Named pipes will throw an error if attempting to connect to a non-existent server. The data must also be converted into a byte format when sent through the pipe, then re-converted back into a usable format in the respective programs.
</p>

<br>
<h2 align="middle">Technical Details - Camera Data Transformation/Conversion</h2>

<p>
OpenVR and Instant-NGP use different coordinate systems. In OpenVR, the camera is oriented toward the negative z direction, positive x is towards the right, and positive y faces up. However, in Instant-NGP, the coordinate system has the y and z directions reversed relative to OpenVR. Therefore, we had to find a way to transform between the two systems to ensure that we could accurately track the Instant-NGP camera’s movements to the movements of the VR headset.
</p>

<p>
Both systems use similar matrix conventions to define the camera positions; they use 3x4 matrices where the first 3 columns define the x, y, z rotation and the 4th column describes an xyz translation from the origin.
</p>
<p>
$$\begin{bmatrix}
  \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & x_{pos}\\
  X_{rot} & Y_{rot} & Z_{rot} &  y_{pos}\\
  \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & \rule[-1ex]{0.5pt}{2.5ex} & z_{pos}\\
\end{bmatrix}$$

</p>
<p>
  When transforming from headset-space to NeRF-space coordinates, we wanted to make sure that the viewer always started in the same orientation in NeRF-space. In order to achieve this, we defined an initial camera matrix that we thought was a good starting point. For us, that was:
</p>
<p>
  $$ \begin{bmatrix}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & 1 & 0
\end{bmatrix} $$
</p>
<p>
where only the first 3 columns represent the rotation. Then, using \(H\) to represent the initial headset rotation and \(C_{init}\) as the initial camera rotation, we can simply find a transform between headset space and NeRF space:
</p>
<p>
  $$
X\cdot H = C_{init}$$
$$
X\cdot H\cdot H^{-1} = C_{init}\cdot H^{-1} \\
$$
$$
X = C_{init}\cdot H^{-1} \\
$$
</p>
<p>
  For every subsequent headset position, we multiply \(X \cdot H\) to get the headset rotation in camera space.
</p>
<p>
  To calculate the translation, we multiply the headset’s xyz position by a passed in scaling factor to account for the difference in scale between the headset coordinate system and the NeRFs. We then concatenate this to the calculated headset rotation in camera-space.
</p>
<p>
  $$
\begin{bmatrix}
  & x_{pos} * t\\
  X\cdot H & y_{pos} * t\\
  & z_{pos}*t
\end{bmatrix}
$$
</p>
<p>
  For one final implementation detail, when processing the view matrix, the Instant-NGP code cycles the rows up by one row in order to convert the matrix into a format understandable by the inner NeRF code. Therefore, we cycle our matrix down by one row to keep the axes in the correct order after processing.
</p>
<p>
  $$
\begin{bmatrix}
  X\cdot H[2] & z_{pos}*t\\
  X\cdot H[0] & y_{pos} * t\\
  X\cdot H[1] & x_{pos} * t
\end{bmatrix}
$$
</p>
<br>
<h2 align="middle">Technical Details - Stereo Imaging</h2>
<p>
To implement stereo imaging, we must calculate a view matrix for each eye and use them to render an image from each eye. We can then send those images to the headset to be viewed as a stereo pair.
</p>
<p>
  We first use the OpenVR method getEyeToHeadTransform() to find the position of each eye relative to the headset position in object space. We then rotate these eye offsets by the headset rotation and add them directly to the headset position to get the eye positions in world space.
</p>
<p>
  To achieve the effect of stereo 3D, we need to converge the view direction of each eye to focus on a common point. This is accomplished by adding a parameter for the focusing distance of the eyes, \(x\). Knowing the Interpupillary Distance, \(IPD\), we can set up a diagram representing the view direction of the eyes. Here, \(f\) is the point at which the eyes are focused, \(\theta\) is the amount by which the eyes must be rotated, and \(l\) and \(r\) are the left and right eye positions respectively.
</p>
<center>
  <img src="./images/IPD.PNG">
</center>
<p>
  We can then solve for \(\theta\) using triangle equations:
</p>
<p>
  $$\theta = \arcsin{(\frac{IPD/2}{\sqrt{(IPD/2)^2 + x^2}})}$$
</p>
<p>
  To converge the eyes we multiply the rotation of the left eye by a rotation matrix about the y axis of \(-\theta\) and the right eye by \(\theta\).
</p>
<p>
  To sync the image for each eye we made sure that the images for each eye were rendered using the same headset pose. On program startup the initial headset pose is first sent to Instant-NGP to calculate the transform matrix. Both eye positions are then calculated and sent to Instant-NGP. These are then used to render two images and sent to OpenVR, and this process continues until program termination.
</p>

<br>
<h2 align="middle">Technical Details - OpenGL Texturing</h2>
<p>
  To use the image bytes obtained from Instant-NGP, the image must first be converted into an OpenGL texture. First an OpenGL context is defined to store the state associated with the OpenGL instance. Then the byte array is converted into a usable structure utilizing multiple for loops. This creates a nested list structure, where the innermost list corresponds to the pixel data, the next list is the image row, and the outermost list is the complete image. The alpha values are discarded during this step.
</p>
<p>
  $$
  \begin{bmatrix}
 r_{1} & g_{1}  & b_{1} & a_{1} & \dots & r_{width*height} & g_{width*height}  & b_{width*height} & a_{width*height} \\
\end{bmatrix}
\to \begin{bmatrix}

\begin{bmatrix}

\begin{bmatrix}
 r_{0} & g_{0} & b_{0}\\
\end{bmatrix}
&
\dots
&
\begin{bmatrix}
 r_{w - 1} \hspace1ex & g_{w-1} \hspace1ex  & b_{w-1}\\
\end{bmatrix}\\
\end{bmatrix}\\
\begin{bmatrix}

\begin{bmatrix}
 r_{w} & g_{w}  & b_{w}\\
\end{bmatrix}
&
\dots
&
\begin{bmatrix}
 r_{2*w - 1} & g_{2*w - 1}  & b_{2*w - 1}\\
\end{bmatrix}
\end{bmatrix}\\

\\
\vdots
\\
\\
\begin{bmatrix}
\begin{bmatrix}
 r_{w*(h-1)} & g_{w*(h-1)}  & b_{w*(h-1)}\\
\end{bmatrix}
&
\dots
&
\begin{bmatrix}
 r_{w*h} & g_{w*h}  & b_{w*h}\\
\end{bmatrix}
\end{bmatrix}\\


\end{bmatrix}
$$
</p>
<p>
  Converting the reorganized data into an OpenGL texture is achieved by calling glTexImage2D() on the image bytes and binding the texture to an OpenGL ID. We account for minification and magnification cases by using a linear sampler.
</p>
<br>
<h2 align="middle">Technical Details - VR Overlay/Compositor</h2>
<p>
  A VR overlay represents a 2D plane set in the 3D scene. In mono-imaging, the NeRF scene is rendered onto a 2D overlay that is positioned to be right in front of the viewing angle. The overlay follows the user’s movements exactly, appearing at the same place in their field of view at each frame.
</p>
<p>
  To project the image onto the overlay we call the OpenVR function SetOverlayTexture() which applies the texture generated in the previous step to an overlay plane. Then to position the overlay correctly we call SetOverlayTransformTrackedDeviceRelative(), which applies a transform to the overlay relative to the headset.
</p>
<p>
  We define a transform that places the overlay just in front of the user’s eyeline at all times. Because the transform is relative to the head, the overlay plane appears in the same spot in the user’s field of view in every frame. Since the 2D image changes in response to user movements in a way that is consistent with 3D perspective, it creates the illusion of a 3D scene.
</p>
<p>
  In stereo imaging, we instead submit an image for each eye to OpenVR’s compositor. Instead of applying a texture to an overlay for the user to view, we apply the texture directly onto each eye display. This allows for two different textures to be viewed simultaneously. Using the compositor does come with some drawbacks, notably if the texture is not updated at a high enough frame rate it will begin flickering.
</p>
<br>
<h2 align="middle">Final Results</h2>

<h3 align="middle">Benchmarks</h3>
<table id="table" border="1">
  <tr>
    <th>Scene</th>
    <th>Mono</th>
    <th>Stereo</th>
  </tr>
  <tr>
    <th>Lego Car</th>
    <th>~4 fps</th>
    <th>~2 fps</th>
  </tr>
  <tr>
    <th>Ficus</th>
    <th>~9 fps</th>
    <th>~4 fps</th>
  </tr>
  <tr>
    <th>Desk</th>
    <th>~3-4 fps</th>
    <th>~1-2 fps</th>
  </tr>
</table>

<br>
<h2 align="middle">Lessons Learned / Issues Faced</h2>
<p>
  We learned that we should ensure that multiple group members have access to equipment that can run our project. At first, only one of our computers was capable of running Instant-NGP, and we only had one VR headset. This reduced our capability to work in parallel / asynchronously.
</p>
<p>
  We also learned that thorough reading of documentation is invaluable! Many issues that we ran into were solved simply by taking a step back, reading the documentation to understand what we were working with, and going from there. For example, at one point we were stuck on getting OpenGL to properly convert the image data sent from InstantNGP into a texture. By reading documentation from both OpenGL and InstantNGP, we figured out that the issue was the format for the pixel data not matching. We were quickly able to fix this issue once we realized it, thanks to the documentation.
</p>
<br>
<h2 align="middle">Future Work</h2>
<p>
Light field rendering for performance - One way to speed up performance could be extracting a learned lightfield from a NeRF, and rendering views from that, rather than continually querying the NeRF for image data.
</p>
<p>
  Adaptive Focusing - currently, our stereo works by orienting the eyes to focus at a fixed distance d, where d is a passed in parameter. Future work could instead adaptively shift focus to different distances over time as the user looks at different objects in space.
</p>
<br>
<h2 align="middle">References</h2>
<a href="https://www.khronos.org/opengl/wiki/Creating_an_OpenGL_Context_(WGL)">OpenGL Context Guide</a><br>
<a href="https://github.com/NVlabs/instant-ngp">Instant-NGP</a><br>
<a href="https://github.com/ValveSoftware/openvr">OpenVR Library and Documentation</a><br>
<a href="https://github.com/cmbruns/pyopenvr">PyOpenVR API</a><br>
<br>
<h2 align="middle">Contributions</h2>
<p>
  Cyrus Hamirani - Contributed to OpenGL texture work, interfacing with InstantNGP’s API, working with OpenVR (generating overlays, compositor, etc), stereo imaging, final writeup, video.
</p>
<p>
  Michael Van Luven - Contributed to OpenGL texture creation, named pipes, interfacing with InstantNGP API, working with OpenVR (generating overlays, compositor, etc), mono imaging, and camera coordinate system, final writeup, video.
</p>
<p>
  Abhik Ahuja - Contributed to camera coordinate system translation, stereo imaging, codebase management, refinements and additional tooling, final writeup, video.
</p>


</body>
</html>
