<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>Final Project</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">Neural Radiance Field with Virtual Reality</h1>
<h2 align="middle">Team Members: Abhik Ahuja, Cyrus Hamirani, Michael Van Luven, Gavin Fure</h2>

<br><br>

<div>

<h2 align="middle">Summary</h2>
<p> 
  The end result of our project will be a VR scene with a user capable of viewing images stitched together with NeRFs that are realistic. We will also add effects to enhance the realism of the VR, including motion blur, depth of field, different camera apertures, and a convergence heatmap.</p>

<h2 align="middle">Problem Description</h2>

<p>
  While Neural Radiance Fields allow a user to generate a continuous scene from a discrete amount of sample images, they are not able to view it with a first-person perspective. Our project will focus on implementing a VR add-on to the NerF model, allowing the user to move around the scene and view objects from different perspectives. The challenging part of this project will be mapping the correct image to the location and angle of the VR headset, as well as ensuring that the user's view is realistic. It may also be difficult to apply motion blur and other post processing to the effect, since NeRF's modeled representations of scenes don't allow direct access to 3D object data. However, it seems that augmentation (like adding objects into a 3D scene) is possible, so we're hopeful that we can at least add some effects or scene modifications. 
</p>


<h2 align="middle">Goals and Deliverables</h2>

<p>
  The images created will be utilizing the continuous set of images generated by a NeRF-like representation to output a first-person perspective of the loaded scene. We may also explore using Plenoxels instead of NeRFs, since they have demonstrated a large speedup in optimization time. <br>
  We will measure the quality of the system by the minimum, average, and maximum framerate in more complex viewing environments. At a minimum the system should be capable of rendering the scene at 90 frames per second with a resolution of 2160 X 1200. Lower frame rates than these will induce disorientation and nausea, diminishing the quality of the experience.
  An additional quality measurement will be more subjective, based on the systemâ€™s ability to provide an output that is seamless, good-looking, and relatively free of noise and artifacts. <br>
  We hope to deliver a live VR demo that will feature a uniform hemisphere with the user placed in the center, allowing the user to move around and view the scene generated by NeRFs. The image supplied to the user will be modified by motion blur and depth of field. Adjustable sliders will control the strength of both effects. If a live demo is not possible because of computation requirements or other constraints, we will present a recording instead.
</p>
<p>
  If the initial system can be quickly implemented, then other addons we plan to include are a slider that the user can modify to increase or decrease the base set of training images supplied to the NeRF. This will display the effect that more training information has on the quality of an environment.
  We also plan to include sliders for camera apertures and a convergence heatmap.
</p>
<p>
  With this project, we hope to answer the following questions:<br>
  1. Is a NeRF VR viewer feasible with our present consumer technology?<br>
  2. How does the quality of the NeRF VR experience compare to more traditional implementations? Is it a suitable replacement?<br>
  3. The NeRF model can construct entire scenes from just a few images, making it fairly easy to collect the data needed to represent a real-world location in VR. Is this benefit strong enough to outweigh the performance costs of this expensive ML model?<br>
  4. What are some potential ways to improve the performance of a NeRF VR viewer?<br>
</p>

<h2 align="middle">Schedule</h2>

<p>
  1st week: A trained NeRF with two scenes, and a function that maps the location of the camera with the trained image.<br>
  2nd week: A barebones working implementation of the demo, mapping the VR location with the NeRF input.<br>
  3rd week: Implementing additional effects as described above, including motion blur, depth of field, camera apertures.<br>
  4th week: Addition of testing features to allow us to determine the efficiency of the system.<br>
</p>

<h2 align="middle">Resources</h2>

<p>
  We're planning on using the <a href="https://developer.vive.com/resources/vive-wave/">HTC VIVE</a> for VR. We hope to be able to render scenes in real time by computing on a commercial GPU. Depending on the feasibility of rendering on our own machines, we were also considering trying to get access to the Open Computing Facility. Below are some papers and other resources that we plan to use.<br>
  <a href="https://arxiv.org/abs/2103.10380">FastNeRF: High-Fidelity Neural Rendering at 200FPS</a><br>
  <a href="https://paperswithcode.com/paper/block-nerf-scalable-large-scene-neural-view">Block-NeRF: Scalable Large Scene Neural View Synthesis</a><br>
  <a href="https://alexyu.net/plenoxels/?s=09">Plenoxels: Radiance Fields without Neural Networks</a><br>
  <a href="https://steantycip.com/blog/the-importance-of-framerates-in-vr">The importance of frame rates in VR</a><br>
  <a href="https://github.com/bmild/nerf">Code release for NeRF (Neural Radiance Fields)</a><br>

</p>


</body>
</html>
